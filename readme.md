Figure I would upload the code I made back in July 2024. 
To beat other people and organziations to the punch, that is the superiority of utilizing the Apple Ecosystem for Private AI.
The possibility of something like Llama 3 8B only on a iPhone 13 Pro, is not far fetched. It is possible, through the amazing optimizations through CoreML.
I have created the code, but I am unable to perform an optimized coreml model due to my macbook 2020 m1, only being 8gb of ram. :(
I was BARELY able to pull off this conversion to CoreML, not optimized. Also a storage issue.


Here is the HF Repo:
https://huggingface.co/anthonymikinka/Meta-Llama-3-8B-Instruct

In this repo will be:

**HF Repo**
   - "code for llama3.py"

**Two Logit Processor Code**
   - logit-processor-llama3-v2.py
   - logit-processor-llama3.py

**Interference Code**
   - "meta-llama-inf.py"
     
**CoreMLOptimization Code Prune and Palletization for the Llama Model**
- Meta-Llama-3-8B-Instruct-Stateful-Pruned-Palettized.py (formerly called untitled.py, as noted in the terminal text file)
- "terminal for llama3.txt"
